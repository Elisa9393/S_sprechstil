# S_sprechstil
Seminar to analyze a speech

In this seminar it was the aim to learn the different steps to analyze speeches in Python. First, it was neccessary to choose a speech. For the soundsfile of Alexa a conversation with the voice assistant was recorded and then Alexa's parts in the dialog were cut out and a coherent wav-file was created ou of these parts.

Then it was subsampled to 16.00 Hertz with Audacity. The nect step was the automatic segmentation e.g. from audeering or voice activated detection or the ina speech segmenter. The ina speech segmenter can also classify biological gender and detect music or noise. It is using tensorflow which is based on a neural network. The ina speech segmenter is segmenting bigger segments because it's not possible to adjust the settings how long the breaks are though. That is why the automatic speech segmentation from audeering was used in this course.

When the wav-file was edited and segmented, the midlevel-descriptors were extracted. The script for that [is here](mid_level_descriptors_aktuell.ipynb)
To explain the term midlevel-descriptors it is neccessary to distinguish between low-level-descriptors and midlevel-descriptors.Low level is the framewise analysis of the features of a speech signal. Low level feature is a series of values for one utterance, which is called functionals. It is possible to take the median, range or mean of these features. Mid level descriptors sum up a series of low level descriptors. They are something in between the whole utterance and low level descriptors, meaning the sillables. Subsequently, they sum up the low level descriptors in sillables. They can be used for example to see how fast someone speaks. 
Low level descriptors return value framewise (each frame 20ms) and these frames overlap all 10 miliseconds. 80 segments of functionals have all 20 ms frames. That's why there are 38000 low level descriptors

For the lowlevel-descriptors to be determined, first the segmented file of the alexa-recording had to be loaded from the pickle file and rearranged. This is important to access the files from the path. it is possible to play a certain segment of the file with the ipython function to test how the audio sounds and if the file was saved correctly. Then the midlevel descriptors are extracted from cache a smile object is used to determine the features.

Additionally, the script accesses the Speech Recognition from Google and transcribes the text. With the transcription it is possible to analyze the text. The Type-Token-Ratio is a way of measuring variation in the vocabulary and shows how rich the lexical variation is. The Type-Token-Ratio for the text of the Alexa-Soundfile is 0.74, which calculated by 100 is 74%. This tells us that Alexa uses varied vocabulary. The linguistic analysis could be continued by this function, for example to find out to which percentage certain parts of speech are used.

To be able to compare the different speeches, the script [here](add_annotations_aktuell.ipynb) was used. It is used to annotate the dataframes for each speaker. This is important to compare the speeches in the next step according to different categories. The categories defined in the script are id, gender, speakertype, age, language, speechtype and the source.

Code to analyze and compare all speeches [is here](large_dataframe_aktuell.ipynb). In this script all the speeches are put together in one directory and a new data frame in created in which which all the pickle files are summed up. In combination with the extraction of the midlevel descriptor features all the speeches can be compared. In the analysis we found out that the different speaker-types are synthetic, explanatory, informative, preaching, persuasive, personal, interview and debate, wheras most speeches are from the group debate. Furthermore, only two of the speeches are synthetic. I extended the analysis by plotting the ages of the speakers in a pie chart. Also the speakers can be represented in a pie chart. Another comparison I did, was plotting the speech rate of the human and the synthetic voices in a boxplot. The result was that the speech rate of the speakertype synthetic and human are similarly fast but the synthetic voices have a larger range around the median. But when investigating the size of the matrices  df_synthetic and df_alexa, this shows that they have the same size. This means that in df_synthetic only df_alexa is contained and subsequently it is only a comparison of alexa and the human voices, since the q voice is not included. I also created a data frame for the id's of all speeches and created a boxplot for the speech rate of all of them. This shows that Hildegard Knef has the highest speech rate, wheras Martin Luther King has the lowest one. I also created a boyplot that compares the pause proportion of the informative and the persuasive speeches. It shows that the mean of the pause proportions for the persuasive speeches is 0.05 higher than the one of the informative speeches.

